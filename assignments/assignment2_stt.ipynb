{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Destructor169/cs-203-stt-ai/blob/main/assignments/assignment2_stt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76eb3311",
      "metadata": {
        "id": "76eb3311"
      },
      "source": [
        "# Assignment 2: The \"Smart Labeling Pipeline\" Challenge\n",
        "\n",
        "**Total Marks: 20**\n",
        "\n",
        "Build a cost-effective, high-quality labeling pipeline using human annotation, programmatic rules, and LLMs.\n",
        "\n",
        "This notebook implements an end-to-end smart labeling pipeline to:\n",
        "1. Establish gold standard through human annotation and measure inter-annotator agreement (6 marks)\n",
        "2. Label data programmatically using weak supervision (Snorkel) (6 marks)\n",
        "3. Optimize labeling budget using active learning (5 marks)\n",
        "4. Leverage LLMs for bulk labeling and detect hallucinations (e.g. noisy labels) (3 marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03665b6c",
      "metadata": {
        "id": "03665b6c"
      },
      "source": [
        "## Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install snorkel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQhqnXEFHFON",
        "outputId": "adf0acec-b3fd-4635-f6df-ccd9368ebe7c"
      },
      "id": "fQhqnXEFHFON",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: snorkel in /usr/local/lib/python3.12/dist-packages (0.10.0)\n",
            "Requirement already satisfied: munkres>=1.0.6 in /usr/local/lib/python3.12/dist-packages (from snorkel) (1.1.4)\n",
            "Requirement already satisfied: numpy>=1.24.0 in /usr/local/lib/python3.12/dist-packages (from snorkel) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from snorkel) (1.16.3)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from snorkel) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.33.0 in /usr/local/lib/python3.12/dist-packages (from snorkel) (4.67.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20.2 in /usr/local/lib/python3.12/dist-packages (from snorkel) (1.6.1)\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from snorkel) (2.9.0+cpu)\n",
            "Requirement already satisfied: tensorboard>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from snorkel) (2.19.0)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from snorkel) (5.29.6)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.12/dist-packages (from snorkel) (3.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->snorkel) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->snorkel) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->snorkel) (2025.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.20.2->snorkel) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.20.2->snorkel) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.13.0->snorkel) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.13.0->snorkel) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.13.0->snorkel) (3.10.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.13.0->snorkel) (26.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.13.0->snorkel) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.13.0->snorkel) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.13.0->snorkel) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.13.0->snorkel) (3.1.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.2.0->snorkel) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.2.0->snorkel) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.2.0->snorkel) (1.14.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.2.0->snorkel) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.2.0->snorkel) (2025.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.2.0->snorkel) (1.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.13.0->snorkel) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae778e21",
      "metadata": {
        "id": "ae778e21"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from snorkel.labeling import labeling_function, PandasLFApplier, LFAnalysis\n",
        "from snorkel.labeling.model import LabelModel\n",
        "from statsmodels.stats.inter_rater import fleiss_kappa\n",
        "import google.generativeai as genai\n",
        "import time\n",
        "from pathlib import Path\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e34d483d",
      "metadata": {
        "id": "e34d483d"
      },
      "source": [
        "## Task 1: The Human as Annotator (6 Marks)\n",
        "\n",
        "**Objective:** Establish a \"Gold Standard\" dataset and measure human consensus.\n",
        "\n",
        "### Part 1.1: Parse Annotator CSV Files\n",
        "\n",
        "After annotating the first 100 reviews, export annotations from three annotators (A, B, C) as CSV files.\n",
        "Parse these CSV files into clean DataFrames for analysis."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "BASE_PATH = \"/content/drive/MyDrive/dataset/STT\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5af6fl_OHPMN",
        "outputId": "b541e776-3975-478b-fb60-7e8d049f2f6e"
      },
      "id": "5af6fl_OHPMN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "290b8d42",
      "metadata": {
        "id": "290b8d42"
      },
      "outputs": [],
      "source": [
        "def parse_annotator_csv(csv_path):\n",
        "    \"\"\"\n",
        "    Parses annotator CSV file into a clean DataFrame.\n",
        "\n",
        "    Args:\n",
        "        csv_path (str): Path to annotator CSV file\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with columns ['review_id', 'review', 'label']\n",
        "                     where label is one of: 'Positive', 'Negative', 'Neutral'\n",
        "\n",
        "    Note:\n",
        "        - Look for relevant column names in the CSV file\n",
        "        - If column names differ, the function will try to map them appropriately\n",
        "        - Finally, return with two columns 'review' and 'label'\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Case 1: Label Studio exports label directly\n",
        "    if {'review', 'sentiment'}.issubset(df.columns):\n",
        "        out = df[['review', 'sentiment']].copy()\n",
        "        out.columns = ['review', 'label']\n",
        "\n",
        "    # Case 2: Label stored inside annotation_result JSON\n",
        "    elif 'annotation_result' in df.columns:\n",
        "        reviews = []\n",
        "        labels = []\n",
        "\n",
        "        for _, row in df.iterrows():\n",
        "            annotation = json.loads(row['annotation_result'])\n",
        "            label = annotation[0]['value']['choices'][0]\n",
        "            reviews.append(row['review'])\n",
        "            labels.append(label)\n",
        "\n",
        "        out = pd.DataFrame({\n",
        "            'review': reviews,\n",
        "            'label': labels\n",
        "        })\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Unrecognized Label Studio CSV format\")\n",
        "\n",
        "    out['review_id'] = range(len(out))\n",
        "    return out[['review_id', 'review', 'label']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81905fd6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "81905fd6",
        "outputId": "ed9451aa-0ac9-4bc1-f620-ba6c58cc7e69"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   review_id                                             review     label\n",
              "0          0  This movie is a triumph in every sense. Highly...  Positive\n",
              "1          1  I have never been so bored in my life. The sco...  Negative\n",
              "2          2  I was completely blown away by this film. The ...  Positive\n",
              "3          3  The trailer was better than the movie. The act...  Negative\n",
              "4          4  Middle of the road entertainment. Visually it'...   Neutral"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-644f84c6-bef7-4211-b0c2-275a27763044\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_id</th>\n",
              "      <th>review</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>This movie is a triumph in every sense. Highly...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>I have never been so bored in my life. The sco...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>I was completely blown away by this film. The ...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>The trailer was better than the movie. The act...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Middle of the road entertainment. Visually it'...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-644f84c6-bef7-4211-b0c2-275a27763044')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-644f84c6-bef7-4211-b0c2-275a27763044 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-644f84c6-bef7-4211-b0c2-275a27763044');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_a",
              "summary": "{\n  \"name\": \"df_a\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"review_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 29,\n        \"min\": 0,\n        \"max\": 99,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          83,\n          53,\n          70\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"I struggled to sit through the first half. A complete misfire.\",\n          \"Ideally, this should have been great, but something was off. It\\u2019s definitely not for everyone.\",\n          \"Great concept, but the execution was clich\\u00e9d. I'm honestly still trying to process what I just saw.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Positive\",\n          \"Negative\",\n          \"Neutral\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# TODO: Parse CSV files (replace with actual file paths)\n",
        "\n",
        "df_a = parse_annotator_csv(f\"{BASE_PATH}/annotator_a.csv\")\n",
        "df_b = parse_annotator_csv(f\"{BASE_PATH}/annotator_b.csv\")\n",
        "df_c = parse_annotator_csv(f\"{BASE_PATH}/annotator_c.csv\")\n",
        "\n",
        "# Display sample data\n",
        "df_a.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cdcf20f",
      "metadata": {
        "id": "2cdcf20f"
      },
      "source": [
        "### Part 1.2: Implement Fleiss' Kappa from Scratch\n",
        "\n",
        "Measure inter-annotator agreement using Fleiss' Kappa statistic.\n",
        "Implement the formula from scratch and compare with statsmodels implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "886fd3e3",
      "metadata": {
        "id": "886fd3e3"
      },
      "outputs": [],
      "source": [
        "def fleiss_kappa_scratch(rating_matrix):\n",
        "    \"\"\"\n",
        "    Computes Fleiss' Kappa for multiple raters from scratch.\n",
        "\n",
        "    Args:\n",
        "        rating_matrix (np.array): A Count Matrix of shape (N, k).\n",
        "                                  - N = number of items (rows)\n",
        "                                  - k = number of categories (columns)\n",
        "                                  - Element [i, j] = Count of raters who assigned category j to item i.\n",
        "                                  Example:\n",
        "                                    [[0, 0, 3],   # Item 0: All 3 raters said Category 2\n",
        "                                     [1, 2, 0]]   # Item 1: 1 rater said Cat 0, 2 said Cat 1\n",
        "\n",
        "\n",
        "    Returns:\n",
        "        float: Kappa score (ranges from -1 to 1, where 1 = perfect agreement)\n",
        "\n",
        "    Formula:\n",
        "        κ = (P_bar - P_e_bar) / (1 - P_e_bar)\n",
        "\n",
        "        where:\n",
        "        - P_bar = (1/N) * Σ(P_i) = average proportion of agreement across all items\n",
        "        - P_i = (1/(n*(n-1))) * Σ(k_ij * (k_ij - 1)) for item i\n",
        "        - P_e_bar = Σ(p_j^2) = expected agreement by chance\n",
        "        - p_j = proportion of all assignments to category j\n",
        "\n",
        "    Note:\n",
        "        - N = number of items (samples)\n",
        "        - n = number of raters per item (should be constant)\n",
        "        - k_ij = number of raters who assigned category j to item i\n",
        "    \"\"\"\n",
        "    N, k = rating_matrix.shape\n",
        "    n = np.sum(rating_matrix[0])  # number of raters\n",
        "\n",
        "    # Calculate P_i for each item\n",
        "    P_i = []\n",
        "    for i in range(N):\n",
        "        row = rating_matrix[i]\n",
        "        P_i.append(np.sum(row * (row - 1)) / (n * (n - 1)))\n",
        "\n",
        "    P_bar = np.mean(P_i)\n",
        "\n",
        "    # Calculate p_j for each category\n",
        "    p_j = np.sum(rating_matrix, axis=0) / (N * n)\n",
        "    P_e_bar = np.sum(p_j ** 2)\n",
        "\n",
        "    # Apply Fleiss' Kappa formula\n",
        "    kappa = (P_bar - P_e_bar) / (1 - P_e_bar)\n",
        "    return kappa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bb440bb",
      "metadata": {
        "id": "4bb440bb"
      },
      "outputs": [],
      "source": [
        "def prepare_rating_matrix(df_a, df_b, df_c):\n",
        "    \"\"\"\n",
        "    Converts three DataFrames into a rating matrix for Fleiss' Kappa calculation.\n",
        "\n",
        "    Args:\n",
        "        df_a, df_b, df_c: DataFrames with columns ['review_id', 'review', 'label']\n",
        "\n",
        "    Returns:\n",
        "        np.array: Rating matrix of shape (N_samples, N_categories)\n",
        "                  where categories are ['Negative', 'Neutral', 'Positive']\n",
        "    \"\"\"\n",
        "    # TODO: Merge the three DataFrames on review\n",
        "    # Hint: Use pd.merge() or pd.concat() with proper keys\n",
        "\n",
        "    merged = df_a.merge(df_b, on='review_id', suffixes=('_a', '_b'))\n",
        "    merged = merged.merge(df_c, on='review_id')\n",
        "    merged.rename(columns={'label': 'label_c'}, inplace=True)\n",
        "\n",
        "    label_map = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
        "    rating_matrix = np.zeros((len(merged), 3), dtype=int)\n",
        "\n",
        "    for i, row in merged.iterrows():\n",
        "        labels = [row['label_a'], row['label_b'], row['label_c']]\n",
        "        for label in labels:\n",
        "            rating_matrix[i, label_map[label]] += 1\n",
        "\n",
        "    # TODO: Return numpy array of shape (N_samples, 3)\n",
        "    # Order: [Negative_count, Neutral_count, Positive_count] for each row\n",
        "    return rating_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Prepare rating matrix and calculate Fleiss' Kappa\n",
        "\n",
        "rating_matrix = prepare_rating_matrix(df_a, df_b, df_c)\n",
        "\n",
        "kappa_scratch = fleiss_kappa_scratch(rating_matrix)\n",
        "kappa_statsmodels = fleiss_kappa(rating_matrix)\n",
        "\n",
        "# TODO: Print the difference between the two implementations\n",
        "print(f\"Fleiss' Kappa (Scratch): {kappa_scratch:.4f}\")\n",
        "print(f\"Fleiss' Kappa (Statsmodels): {kappa_statsmodels:.4f}\")\n",
        "print(f\"Difference: {abs(kappa_scratch - kappa_statsmodels):.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bBdNWTLYeF-",
        "outputId": "12c34b45-5e07-4371-e752-3a61f04733df"
      },
      "id": "1bBdNWTLYeF-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fleiss' Kappa (Scratch): 0.8159\n",
            "Fleiss' Kappa (Statsmodels): 0.8159\n",
            "Difference: 0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9407cec1",
      "metadata": {
        "id": "9407cec1"
      },
      "source": [
        "### Part 1.3: Conflict Resolution\n",
        "\n",
        "Identify conflicts where annotators disagree and resolve them using majority vote.\n",
        "For complete ties (all three differ), default to 'Neutral'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5aed962c",
      "metadata": {
        "id": "5aed962c"
      },
      "outputs": [],
      "source": [
        "def resolve_conflicts(df_a, df_b, df_c):\n",
        "    \"\"\"\n",
        "    Merges annotations from 3 annotators, resolves disagreements using Majority Vote,\n",
        "    and handles complete ties by defaulting to 'Neutral'.\n",
        "\n",
        "    Args:\n",
        "        df_a, df_b, df_c: DataFrames from each annotator with columns ['review', 'label']\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Final DataFrame with resolved labels (gold standard)\n",
        "                     Columns: ['review', 'label']\n",
        "\n",
        "    Logic:\n",
        "        - Majority Vote: If 2 annotators agree, use their label\n",
        "        - Tie-Breaker: If all 3 differ (e.g., Positive vs. Negative vs. Neutral), assign 'Neutral'\n",
        "    \"\"\"\n",
        "    merged = df_a.merge(df_b, on='review_id', suffixes=('_a', '_b'))\n",
        "    merged = merged.merge(df_c, on='review_id')\n",
        "    merged.rename(columns={'label': 'label_c'}, inplace=True)\n",
        "\n",
        "    final_labels = []\n",
        "\n",
        "    for _, row in merged.iterrows():\n",
        "        labels = [row['label_a'], row['label_b'], row['label_c']]\n",
        "        counts = pd.Series(labels).value_counts()\n",
        "\n",
        "        if counts.iloc[0] >= 2:\n",
        "            final_labels.append(counts.index[0])\n",
        "        else:\n",
        "            final_labels.append('Neutral')\n",
        "\n",
        "    merged['label'] = final_labels\n",
        "    return merged[['review', 'label']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4eec0f3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "c4eec0f3",
        "outputId": "b7a7ad07-90f8-4668-e442-e96e158e11ad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               review  label_a   label_b  \\\n",
              "10  I oscillated between loving and hating this fi...  Neutral  Positive   \n",
              "20  I have mixed feelings about this one. You won'...  Neutral  Negative   \n",
              "29  Great concept, but the execution was jarring. ...  Neutral  Positive   \n",
              "30  Ideally, this should have been great, but some...  Neutral  Negative   \n",
              "32  This is a very difficult movie to categorize. ...  Neutral  Negative   \n",
              "\n",
              "    label_c  \n",
              "10  Neutral  \n",
              "20  Neutral  \n",
              "29  Neutral  \n",
              "30  Neutral  \n",
              "32  Neutral  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-660d7b2a-6d36-4dbb-9cd1-58c6c8b0226f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>label_a</th>\n",
              "      <th>label_b</th>\n",
              "      <th>label_c</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>I oscillated between loving and hating this fi...</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>I have mixed feelings about this one. You won'...</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>Great concept, but the execution was jarring. ...</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>Ideally, this should have been great, but some...</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>This is a very difficult movie to categorize. ...</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Negative</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-660d7b2a-6d36-4dbb-9cd1-58c6c8b0226f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-660d7b2a-6d36-4dbb-9cd1-58c6c8b0226f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-660d7b2a-6d36-4dbb-9cd1-58c6c8b0226f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"conflicts[['review', 'label_a', 'label_b', 'label_c']]\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"I have mixed feelings about this one. You won't remember it a week from now.\",\n          \"This is a very difficult movie to categorize. The first half was phenomenal, but the ending was cheap. It\\u2019s definitely not for everyone.\",\n          \"Great concept, but the execution was jarring. It\\u2019s a weird mix of brilliance and stupidity.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label_a\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Neutral\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label_b\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Negative\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label_c\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Neutral\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# TODO: Display 5 examples of conflicting reviews (if <5 reviews, show all)\n",
        "# Show what A, B, and C each said, and the final resolved label\n",
        "\n",
        "conflicts = df_a.merge(df_b, on='review_id', suffixes=('_a', '_b'))\n",
        "conflicts = conflicts.merge(df_c, on='review_id')\n",
        "conflicts.rename(columns={'label': 'label_c'}, inplace=True)\n",
        "\n",
        "conflicts = conflicts[\n",
        "    ~(\n",
        "        (conflicts['label_a'] == conflicts['label_b']) &\n",
        "        (conflicts['label_b'] == conflicts['label_c'])\n",
        "    )\n",
        "]\n",
        "\n",
        "conflicts[['review', 'label_a', 'label_b', 'label_c']].head(5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Save gold standard to CSV\n",
        "\n",
        "gold_standard_df = resolve_conflicts(df_a, df_b, df_c)\n",
        "gold_standard_df.to_csv(f\"{BASE_PATH}/gold_standard_100.csv\", index=False)\n",
        "\n",
        "gold_standard_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "fnBUlQrdYz74",
        "outputId": "c80f95c9-24f0-481c-e8dc-c2fd2ff7b091"
      },
      "id": "fnBUlQrdYz74",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review     label\n",
              "0  This movie is a triumph in every sense. Highly...  Positive\n",
              "1  I have never been so bored in my life. The sco...  Negative\n",
              "2  I was completely blown away by this film. The ...  Positive\n",
              "3  The trailer was better than the movie. The act...  Negative\n",
              "4  Middle of the road entertainment. Visually it'...   Neutral"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3c69e13c-9a6f-4e7d-b2c1-2624504aef61\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>This movie is a triumph in every sense. Highly...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I have never been so bored in my life. The sco...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I was completely blown away by this film. The ...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The trailer was better than the movie. The act...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Middle of the road entertainment. Visually it'...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3c69e13c-9a6f-4e7d-b2c1-2624504aef61')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3c69e13c-9a6f-4e7d-b2c1-2624504aef61 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3c69e13c-9a6f-4e7d-b2c1-2624504aef61');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "gold_standard_df",
              "summary": "{\n  \"name\": \"gold_standard_df\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"I struggled to sit through the first half. A complete misfire.\",\n          \"Ideally, this should have been great, but something was off. It\\u2019s definitely not for everyone.\",\n          \"Great concept, but the execution was clich\\u00e9d. I'm honestly still trying to process what I just saw.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Positive\",\n          \"Negative\",\n          \"Neutral\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba2645b7",
      "metadata": {
        "id": "ba2645b7"
      },
      "source": [
        "## Task 2: Weak Supervision (The \"Lazy\" Labeler) (6 Marks)\n",
        "\n",
        "**Objective:** Label the next 200 reviews programmatically to save time.\n",
        "\n",
        "### Part 2.1: Heuristic Development\n",
        "\n",
        "Analyze patterns in the gold standard and write at least 3 heuristic functions.\n",
        "Apply them to the remaining 200 unlabeled reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e06eca2d",
      "metadata": {
        "id": "e06eca2d"
      },
      "outputs": [],
      "source": [
        "# Constants for labeling functions\n",
        "POSITIVE = 1\n",
        "NEGATIVE = 0\n",
        "NEUTRAL = 2\n",
        "ABSTAIN = -1\n",
        "\n",
        "# TODO: Load gold standard to analyze patterns\n",
        "\n",
        "\n",
        "# TODO: Analyze patterns (e.g., common positive/negative words, review length, etc.)\n",
        "# This will help you design effective heuristics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a0681ea",
      "metadata": {
        "id": "3a0681ea"
      },
      "source": [
        "### Part 2.2: Snorkel Labeling Functions\n",
        "\n",
        "Wrap your heuristics as Snorkel @labeling_function decorators.\n",
        "Each function should return POSITIVE (1), NEGATIVE (0), NEUTRAL (2), or ABSTAIN (-1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6a3bfbe",
      "metadata": {
        "id": "b6a3bfbe"
      },
      "outputs": [],
      "source": [
        "@labeling_function()\n",
        "def lf_keyword_great(x):\n",
        "    \"\"\"\n",
        "    Example labeling function: Check if \"great\" appears in the review.\n",
        "    Returns POSITIVE if found, otherwise ABSTAIN.\n",
        "    \"\"\"\n",
        "    # TODO: Check if \"great\" (case-insensitive) is in x.review\n",
        "    # Return POSITIVE if found, ABSTAIN otherwise\n",
        "    pass\n",
        "\n",
        "@labeling_function()\n",
        "def lf_short_review(x):\n",
        "    \"\"\"\n",
        "    Label based on review length.\n",
        "    Very short reviews might be neutral or indicate lack of engagement.\n",
        "    \"\"\"\n",
        "    # TODO: Implement logic based on review length\n",
        "    # Return appropriate label (NEUTRAL for very short, or ABSTAIN)\n",
        "    pass\n",
        "\n",
        "@labeling_function()\n",
        "def lf_regex_bad(x):\n",
        "    \"\"\"\n",
        "    Use regex to find negative patterns.\n",
        "    Look for words like \"horrible\", \"terrible\", \"awful\", etc.\n",
        "    \"\"\"\n",
        "    # TODO: Use regex or string matching to find negative keywords\n",
        "    # Return NEGATIVE if found, ABSTAIN otherwise\n",
        "    pass\n",
        "\n",
        "# TODO: Write at least 3 more labeling functions (minimum 6 total)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ab62dc4",
      "metadata": {
        "id": "0ab62dc4"
      },
      "source": [
        "### Part 2.3: Apply Labeling Functions and Analyze Coverage\n",
        "\n",
        "Apply all labeling functions to the 200 unlabeled reviews and calculate coverage and conflict rates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e11384ed",
      "metadata": {
        "id": "e11384ed"
      },
      "outputs": [],
      "source": [
        "def analyze_weak_labels(L_matrix, lfs):\n",
        "    \"\"\"\n",
        "    Prints Coverage and Conflict statistics for the Labeling Functions.\n",
        "\n",
        "    Args:\n",
        "        L_matrix (np.array): Label matrix of shape (N_samples, N_functions)\n",
        "                            Each column represents one labeling function's outputs\n",
        "                            Values: POSITIVE (1), NEGATIVE (0), NEUTRAL (2), ABSTAIN (-1)\n",
        "        lfs: List of labeling functions (for display names)\n",
        "\n",
        "    Metrics to calculate:\n",
        "        - Coverage: Percentage of non-abstain votes per LF\n",
        "        - Conflict Rate: Percentage of samples where LFs disagree\n",
        "    \"\"\"\n",
        "    # TODO: Calculate coverage for each labeling function\n",
        "    # Coverage = (number of non-abstain votes) / (total samples) * 100\n",
        "\n",
        "\n",
        "    # TODO: Calculate conflict rate\n",
        "    # Conflict occurs when multiple LFs label the same sample differently\n",
        "    # Conflict Rate = (number of conflicting samples) / (total samples) * 100\n",
        "\n",
        "\n",
        "    # TODO: Print statistics in a readable format\n",
        "    # Hint: Use LFAnalysis from snorkel for detailed stats (optional)\n",
        "    # Or print manually: LF name, Coverage %, Conflicts count\n",
        "\n",
        "    pass\n",
        "\n",
        "# TODO: Load the 200 unlabeled reviews (you can load the entire dataset and then filter as per the requirement)\n",
        "\n",
        "\n",
        "# TODO: Apply all labeling functions to create L_matrix\n",
        "# lfs = [lf_keyword_great, lf_short_review, lf_regex_bad, ...]  # Add all your LFs\n",
        "# applier = <put your code here>\n",
        "# L_matrix = <put your code here>\n",
        "\n",
        "# TODO: Analyze coverage and conflicts\n",
        "\n",
        "\n",
        "# TODO: Use LFAnalysis for detailed statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "552be82e",
      "metadata": {
        "id": "552be82e"
      },
      "source": [
        "### Part 2.4: Majority Vote Adjudication\n",
        "\n",
        "Use majority vote to generate probabilistic labels (weak labels) for the 200 reviews.\n",
        "Save the result to `weak_labels_200.csv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "523e974f",
      "metadata": {
        "id": "523e974f"
      },
      "outputs": [],
      "source": [
        "# TODO: Train LabelModel to get probabilistic labels\n",
        "\n",
        "\n",
        "# TODO: Convert numeric labels to match your label scheme\n",
        "# Label mapping: 0 -> 'Negative' (or 0), 1 -> 'Positive' (or 1), 2 -> 'Neutral' (or 2), -1 -> 'Abstain'\n",
        "\n",
        "\n",
        "# TODO: Create DataFrame with reviews and weak labels\n",
        "\n",
        "\n",
        "# TODO: Save to CSV"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a2087d6",
      "metadata": {
        "id": "7a2087d6"
      },
      "source": [
        "## Task 3: Active Learning (The Budget Optimizer) (5 Marks)\n",
        "\n",
        "**Objective:** Simulate cost savings by training a model iteratively.\n",
        "\n",
        "### Part 3.1: Query Strategy Implementation\n",
        "\n",
        "Implement Least Confidence and Entropy Sampling from scratch.\n",
        "These strategies select the most informative samples for labeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0a236fc",
      "metadata": {
        "id": "b0a236fc"
      },
      "outputs": [],
      "source": [
        "def least_confidence_sampling(model, X_pool, n_instances=10):\n",
        "    \"\"\"\n",
        "    Selects samples where the model is least confident (uncertainty sampling).\n",
        "\n",
        "    Args:\n",
        "        model: Trained classifier with predict_proba() method\n",
        "        X_pool: Feature matrix of unlabeled samples\n",
        "        n_instances: Number of samples to select\n",
        "\n",
        "    Returns:\n",
        "        np.array: Indices of selected samples\n",
        "\n",
        "    Strategy:\n",
        "        Uncertainty = 1 - max(probability) across all classes\n",
        "        For 3-class classification: Get probabilities for [Negative, Positive, Neutral]\n",
        "        Select samples with highest uncertainty (lowest max probability)\n",
        "    \"\"\"\n",
        "    # Get probability predictions from model\n",
        "    probs = model.predict_proba(X_pool)\n",
        "\n",
        "    # Calculate uncertainty: 1 - max(probability) for each sample\n",
        "    uncertainty = 1 - np.max(probs, axis=1)\n",
        "\n",
        "    # Select top n_instances samples with highest uncertainty\n",
        "    # We want indices of the largest uncertainty values\n",
        "    return np.argsort(uncertainty)[-n_instances:]\n",
        "\n",
        "def entropy_sampling(model, X_pool, n_instances=10):\n",
        "    \"\"\"\n",
        "    Selects samples with highest entropy (information gain).\n",
        "\n",
        "    Args:\n",
        "        model: Trained classifier with predict_proba() method\n",
        "        X_pool: Feature matrix of unlabeled samples\n",
        "        n_instances: Number of samples to select\n",
        "\n",
        "    Returns:\n",
        "        np.array: Indices of selected samples\n",
        "\n",
        "    Strategy:\n",
        "        Entropy = -sum(p * log(p)) for all classes\n",
        "        For 3-class classification: Calculate entropy across [Negative, Positive, Neutral] probabilities\n",
        "        Select samples with highest entropy (most uncertain across all classes)\n",
        "    \"\"\"\n",
        "    # Get probability predictions from model\n",
        "    probs = model.predict_proba(X_pool)\n",
        "\n",
        "    # Calculate entropy: -sum(p * log(p)) for each sample\n",
        "    # Add small epsilon (1e-9) to avoid log(0) errors\n",
        "    epsilon = 1e-9\n",
        "    entropy = -np.sum(probs * np.log(probs + epsilon), axis=1)\n",
        "\n",
        "    # Select top n_instances samples with highest entropy\n",
        "    return np.argsort(entropy)[-n_instances:]\n",
        "\n",
        "def random_sampling(model, X_pool, n_instances=10):\n",
        "    \"\"\"\n",
        "    Baseline strategy: Selects random samples.\n",
        "\n",
        "    Args:\n",
        "        model: Not used, but kept for interface consistency\n",
        "        X_pool: Feature matrix of unlabeled samples\n",
        "        n_instances: Number of samples to select\n",
        "\n",
        "    Returns:\n",
        "        np.array: Randomly selected indices\n",
        "    \"\"\"\n",
        "    # Randomly select n_instances indices from X_pool\n",
        "    if n_instances > X_pool.shape[0]:\n",
        "        n_instances = X_pool.shape[0]\n",
        "    return np.random.choice(X_pool.shape[0], size=n_instances, replace=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1af4c33",
      "metadata": {
        "id": "b1af4c33"
      },
      "source": [
        "### Part 3.2: Data Processing and Setup\n",
        "\n",
        "Load the gold standard (seed) and weak labels (pool).\n",
        "Create a static test set from the pool for evaluation.\n",
        "Vectorize text data using TF-IDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e76e610",
      "metadata": {
        "id": "8e76e610"
      },
      "outputs": [],
      "source": [
        "def load_and_process_data():\n",
        "    \"\"\"\n",
        "    Loads and processes data for active learning.\n",
        "    Checks 'data/' directory first, then current directory.\n",
        "    If CSV files are missing, generates dummy data from Movie_review.xlsx or Movie_reviews.csv.\n",
        "\n",
        "    Returns:\n",
        "        Tuple: (X_seed, y_seed, X_pool, y_pool, X_test, y_test, vectorizer)\n",
        "    \"\"\"\n",
        "    import os\n",
        "    import numpy as np\n",
        "\n",
        "    # Define paths\n",
        "    paths_to_check = ['data/', './']\n",
        "\n",
        "    seed_path = None\n",
        "    pool_path = None\n",
        "\n",
        "    # 1. Look for gold_standard_100.csv\n",
        "    for p in paths_to_check:\n",
        "        if os.path.exists(os.path.join(p, 'data/gold_standard_100.csv')):\n",
        "            seed_path = os.path.join(p, 'data/gold_standard_100.csv')\n",
        "            break\n",
        "\n",
        "    # 2. Look for weak_labels_200.csv\n",
        "    for p in paths_to_check:\n",
        "        if os.path.exists(os.path.join(p, 'data/weak_labels_200.csv')):\n",
        "            pool_path = os.path.join(p, 'data/weak_labels_200.csv')\n",
        "            break\n",
        "\n",
        "    # If seed exists but pool missing, maybe we can generate pool from Movie_reviews.csv in data/\n",
        "    if seed_path and not pool_path:\n",
        "        print(\"Found seed data but missing weak labels. Checking for Movie_reviews.csv to generate pool...\")\n",
        "        movie_review_path = None\n",
        "        for p in paths_to_check:\n",
        "             if os.path.exists(os.path.join(p, 'Movie_reviews.csv')):\n",
        "                 movie_review_path = os.path.join(p, 'Movie_reviews.csv')\n",
        "                 break\n",
        "\n",
        "        if movie_review_path:\n",
        "             df_all = pd.read_csv(movie_review_path)\n",
        "             # Assumption: gold standard used first 100. Weak labels use next 200.\n",
        "             df_weak = df_all.iloc[100:300].copy()\n",
        "\n",
        "             # Check for labels, if missing, generate random for simulation\n",
        "             if 'label' not in df_weak.columns and 'sentiment' not in df_weak.columns:\n",
        "                 print(\"  > No labels found in Movie_reviews.csv. Generating RANDOM labels for simulation.\")\n",
        "                 labels = ['Positive', 'Negative', 'Neutral']\n",
        "                 df_weak['label'] = np.random.choice(labels, size=len(df_weak))\n",
        "             elif 'label' not in df_weak.columns and 'sentiment' in df_weak.columns:\n",
        "                 df_weak['label'] = df_weak['sentiment']\n",
        "\n",
        "             # Save locally\n",
        "             df_weak.to_csv('data/weak_labels_200.csv', index=False)\n",
        "             pool_path = 'data/weak_labels_200.csv'\n",
        "             print(\"Generated temporary 'data/weak_labels_200.csv'\")\n",
        "\n",
        "    # Fallback to Movie_review.xlsx if CSV missing\n",
        "    if not seed_path or not pool_path:\n",
        "        print(\"Required CSV files not found. Checking for Movie_review.xlsx to generate dummy data...\")\n",
        "        if os.path.exists('Movie_review.xlsx'):\n",
        "            print(\"Generating dummy data from Movie_review.xlsx...\")\n",
        "            df = pd.read_excel('Movie_review.xlsx')\n",
        "\n",
        "            # Create dummy gold standard (first 100)\n",
        "            df_gold = df.iloc[:100].copy()\n",
        "            labels = ['Positive', 'Negative', 'Neutral']\n",
        "            df_gold['label'] = np.random.choice(labels, size=len(df_gold))\n",
        "            df_gold.to_csv('data/gold_standard_100.csv', index=False)\n",
        "            seed_path = 'data/gold_standard_100.csv'\n",
        "\n",
        "            # Create dummy weak labels (next 200)\n",
        "            df_weak = df.iloc[100:300].copy()\n",
        "            df_weak['label'] = np.random.choice(labels, size=len(df_weak))\n",
        "            df_weak.to_csv('data/weak_labels_200.csv', index=False)\n",
        "            pool_path = 'data/weak_labels_200.csv'\n",
        "            print(\"Dummy files created.\")\n",
        "        else:\n",
        "            print(\"Error: 'Movie_review.xlsx' not found. Cannot generate data.\")\n",
        "            return None\n",
        "\n",
        "    print(f\"Loading seed from: {seed_path}\")\n",
        "    print(f\"Loading pool from: {pool_path}\")\n",
        "\n",
        "    df_seed = pd.read_csv(seed_path)\n",
        "    df_pool_full = pd.read_csv(pool_path)\n",
        "\n",
        "    # Ensure both have 'review' column\n",
        "    if 'review' not in df_seed.columns and 'text' in df_seed.columns:\n",
        "         df_seed.rename(columns={'text': 'review'}, inplace=True)\n",
        "    if 'review' not in df_pool_full.columns and 'text' in df_pool_full.columns:\n",
        "         df_pool_full.rename(columns={'text': 'review'}, inplace=True)\n",
        "\n",
        "    # Handle both 'label' and 'sentiment' column names\n",
        "    label_col_seed = 'label' if 'label' in df_seed.columns else 'sentiment'\n",
        "    label_col_pool = 'label' if 'label' in df_pool_full.columns else 'sentiment'\n",
        "\n",
        "    # Map text labels to numeric: Positive=1, Negative=0, Neutral=2\n",
        "    label_mapping = {\n",
        "        'Positive': 1, 'positive': 1, 'POSITIVE': 1, 'Pos': 1,\n",
        "        'Negative': 0, 'negative': 0, 'NEGATIVE': 0, 'Neg': 0,\n",
        "        'Neutral': 2, 'neutral': 2, 'NEUTRAL': 2, 'Neu': 2,\n",
        "        0: 0, 1: 1, 2: 2\n",
        "    }\n",
        "\n",
        "    # Convert seed labels\n",
        "    df_seed['sentiment_numeric'] = df_seed[label_col_seed].map(label_mapping).fillna(2).astype(int)\n",
        "\n",
        "    # Convert pool labels\n",
        "    df_pool_full['sentiment_numeric'] = df_pool_full[label_col_pool].map(label_mapping).fillna(2).astype(int)\n",
        "\n",
        "    # Create static test set (hold out 50 samples from pool)\n",
        "    if len(df_pool_full) > 50:\n",
        "         df_pool, df_test = train_test_split(df_pool_full, test_size=50, random_state=42)\n",
        "    else:\n",
        "         df_pool = df_pool_full\n",
        "         df_test = df_pool_full.copy() # Fallback if too small\n",
        "\n",
        "    # Vectorize text data using TfidfVectorizer\n",
        "    vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
        "    all_text = pd.concat([df_seed['review'], df_pool['review'], df_test['review']])\n",
        "    vectorizer.fit(all_text.fillna(\"\"))\n",
        "\n",
        "    # Transform datasets to feature matrices\n",
        "    X_seed = vectorizer.transform(df_seed['review'].fillna(\"\")).toarray()\n",
        "    X_pool = vectorizer.transform(df_pool['review'].fillna(\"\")).toarray()\n",
        "    X_test = vectorizer.transform(df_test['review'].fillna(\"\")).toarray()\n",
        "\n",
        "    # Extract numeric labels\n",
        "    y_seed = df_seed['sentiment_numeric'].values\n",
        "    y_pool = df_pool['sentiment_numeric'].values\n",
        "    y_test = df_test['sentiment_numeric'].values\n",
        "\n",
        "    return X_seed, y_seed, X_pool, y_pool, X_test, y_test, vectorizer\n",
        "\n",
        "# Load data\n",
        "data = load_and_process_data()\n",
        "if data:\n",
        "    X_seed, y_seed, X_pool, y_pool, X_test, y_test, vectorizer = data\n",
        "    print(f\"Seed Size: {len(y_seed)}\")\n",
        "    print(f\"Pool Size: {len(y_pool)}\")\n",
        "    print(f\"Test Size: {len(y_test)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd86c256",
      "metadata": {
        "id": "dd86c256"
      },
      "source": [
        "### Part 3.3: Active Learning Loop\n",
        "\n",
        "Implement the iterative active learning loop:\n",
        "1. Train model on current training set\n",
        "2. Query uncertain samples from pool\n",
        "3. \"Label\" them (reveal ground truth)\n",
        "4. Add to training set and retrain\n",
        "5. Log test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bbc766d",
      "metadata": {
        "id": "8bbc766d"
      },
      "outputs": [],
      "source": [
        "def run_active_learning_loop(X_seed, y_seed, X_pool, y_pool, X_test, y_test,\n",
        "                             strategy_func, steps=5, batch_size=10):\n",
        "    \"\"\"\n",
        "    Simulates the active learning loop (matches lab approach).\n",
        "    \"\"\"\n",
        "    # Initialize training set with seed data\n",
        "    X_train = X_seed.copy()\n",
        "    y_train = y_seed.copy()\n",
        "\n",
        "    # Create working copies of pool\n",
        "    X_pool_curr = X_pool.copy()\n",
        "    y_pool_curr = y_pool.copy()\n",
        "\n",
        "    # Initialize history\n",
        "    accuracy_history = []\n",
        "    n_labels_history = []\n",
        "\n",
        "    # Train initial model\n",
        "    model = LogisticRegression( solver='lbfgs', max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate initial model\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    accuracy_history.append(acc)\n",
        "    n_labels_history.append(len(y_train))\n",
        "    # print(f\"Initial: Labels={len(y_train)}, Test Accuracy={acc:.4f}\")\n",
        "\n",
        "    # Iterative loop\n",
        "    for i in range(steps):\n",
        "        if len(y_pool_curr) == 0:\n",
        "            break\n",
        "\n",
        "        # 1. Query\n",
        "        current_batch_size = min(batch_size, len(y_pool_curr))\n",
        "        query_indices = strategy_func(model, X_pool_curr, current_batch_size)\n",
        "\n",
        "        # 2. Label\n",
        "        X_new = X_pool_curr[query_indices]\n",
        "        y_new = y_pool_curr[query_indices]\n",
        "\n",
        "        # 3. Add to training\n",
        "        X_train = np.vstack((X_train, X_new))\n",
        "        y_train = np.concatenate((y_train, y_new))\n",
        "\n",
        "        # 4. Remove from pool\n",
        "        mask = np.ones(len(y_pool_curr), dtype=bool)\n",
        "        mask[query_indices] = False\n",
        "        X_pool_curr = X_pool_curr[mask]\n",
        "        y_pool_curr = y_pool_curr[mask]\n",
        "\n",
        "        # 5. Retrain\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # 6. Evaluate\n",
        "        y_pred = model.predict(X_test)\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "        # 7. Log\n",
        "        accuracy_history.append(acc)\n",
        "        n_labels_history.append(len(y_train))\n",
        "        # print(f\"Step {i+1}: Labels={len(y_train)}, Test Accuracy={acc:.4f}\")\n",
        "\n",
        "    return n_labels_history, accuracy_history\n",
        "\n",
        "if 'X_seed' in locals():\n",
        "    print(\"Running Least Confidence Sampling...\")\n",
        "    n_labels_lc, acc_lc = run_active_learning_loop(X_seed, y_seed, X_pool, y_pool, X_test, y_test, least_confidence_sampling)\n",
        "    print(\"Least Confidence Sampling Completed.\")\n",
        "else:\n",
        "    print(\"Data not loaded. Skipping execution.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5c2408d",
      "metadata": {
        "id": "f5c2408d"
      },
      "source": [
        "### Part 3.4: Visualization and Comparison\n",
        "\n",
        "Plot learning curves comparing Active Learning vs. Random Sampling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cc60750",
      "metadata": {
        "id": "6cc60750"
      },
      "outputs": [],
      "source": [
        "# Run active learning with random sampling (baseline)\n",
        "if 'X_seed' in locals():\n",
        "    print(\"Running Random Sampling...\")\n",
        "    n_labels_random, acc_random = run_active_learning_loop(X_seed, y_seed, X_pool, y_pool, X_test, y_test, random_sampling)\n",
        "\n",
        "    # Plot learning curves\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(n_labels_lc, acc_lc, marker='o', label='Least Confidence')\n",
        "    plt.plot(n_labels_random, acc_random, marker='s', label='Random Sampling')\n",
        "    plt.xlabel('Number of Training Labels')\n",
        "    plt.ylabel('Test Accuracy')\n",
        "    plt.title('Active Learning Learning Curves')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Print comparison summary\n",
        "    print(f\"Final Accuracy (Least Confidence): {acc_lc[-1]:.4f}\")\n",
        "    print(f\"Final Accuracy (Random Sampling): {acc_random[-1]:.4f}\")\n",
        "else:\n",
        "    print(\"Data not loaded. Skipping comparison.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3633624",
      "metadata": {
        "id": "a3633624"
      },
      "source": [
        "## Task 4: AI vs. AI (LLM & Noise Detection) (3 Marks)\n",
        "\n",
        "**Objective:** Use LLMs for bulk labeling and detect hallucinations.\n",
        "\n",
        "**Note:**\n",
        "\n",
        "- Make an account at [open-router](https://openrouter.ai/) and get the API key.\n",
        "- Use `google/gemini-2.5-flash-lite` (free tier) model as your LLM. Read the documentation on how to use it [here](https://openrouter.ai/google/gemini-2.5-flash-lite/api)\n",
        "- Set environment variable using .env file and paste your API key in it.\n",
        "\n",
        "### Part 4.1: LLM Pipeline with Few-Shot Prompting\n",
        "\n",
        "Design a few-shot prompt with 3 examples from gold standard.\n",
        "Send remaining unlabeled samples (~150) to Gemini API for labeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d313c225",
      "metadata": {
        "id": "d313c225"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "import requests\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "API_KEY = os.getenv('OPENROUTER_API_KEY')\n",
        "SITE_URL = \"http://localhost:8000\"  #for OpenRouter rankings\n",
        "SITE_NAME = \"Student Lab Assignment\"\n",
        "\n",
        "MODEL_NAME = \"google/gemini-2.5-flash-lite\"\n",
        "\n",
        "if not API_KEY:\n",
        "    print(\"⚠ Warning: OPENROUTER_API_KEY not found. Please check your .env file.\")\n",
        "\n",
        "\n",
        "def generate_few_shot_prompt(review_text, examples):\n",
        "    \"\"\"\n",
        "    Constructs a few-shot prompt with 3 gold examples + target review.\n",
        "\n",
        "    Args:\n",
        "        review_text (str): The review to be labeled\n",
        "        examples (list): List of 3 example dictionaries with 'review' and 'label' keys\n",
        "\n",
        "    Returns:\n",
        "        str: Formatted prompt string\n",
        "    \"\"\"\n",
        "    prompt = \"Classify the sentiment of the following movie reviews as Positive, Negative, or Neutral.\\n\\n\"\n",
        "    for ex in examples:\n",
        "        prompt += f\"Review: {ex['review']}\\nSentiment: {ex['label']}\\n\\n\"\n",
        "    prompt += f\"Review: {review_text}\\nSentiment: \"\n",
        "    return prompt\n",
        "\n",
        "def query_openrouter(review_text, examples):\n",
        "    \"\"\"\n",
        "    Sends request to OpenRouter API with retry logic and parsing.\n",
        "\n",
        "    Args:\n",
        "        review_text (str): Review to classify\n",
        "        examples (list): Few-shot examples (list of dicts with 'review' and 'label')\n",
        "\n",
        "    Returns:\n",
        "        str: Label ('Positive', 'Negative', or 'Neutral')\n",
        "             Returns None if API fails or response is invalid\n",
        "\n",
        "    Note:\n",
        "        - Uses OpenRouter API endpoint: https://openrouter.ai/api/v1/chat/completions\n",
        "        - Implements retry logic for rate limit errors (429)\n",
        "        - Parses response from OpenRouter's chat completions format\n",
        "    \"\"\"\n",
        "    if not API_KEY: return \"Neutral\"\n",
        "\n",
        "    url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
        "\n",
        "    # Set up headers:\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
        "        \"HTTP-Referer\": SITE_URL,\n",
        "        \"X-Title\": SITE_NAME,\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    # Generate prompt using generate_few_shot_prompt()\n",
        "    prompt = generate_few_shot_prompt(review_text, examples)\n",
        "\n",
        "    # Create payload dictionary:\n",
        "    payload = {\n",
        "        \"model\": MODEL_NAME,\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
        "    }\n",
        "\n",
        "    # Implement retry logic:\n",
        "    for attempt in range(3):\n",
        "        try:\n",
        "            response = requests.post(url, headers=headers, json=payload, timeout=20)\n",
        "            if response.status_code == 200:\n",
        "                # Parse successful response:\n",
        "                content = response.json()['choices'][0]['message']['content'].strip()\n",
        "                if 'positive' in content.lower(): return 'Positive'\n",
        "                if 'negative' in content.lower(): return 'Negative'\n",
        "                if 'neutral' in content.lower(): return 'Neutral'\n",
        "                return content\n",
        "            elif response.status_code == 429:\n",
        "                time.sleep(2)\n",
        "            else:\n",
        "                break\n",
        "        except:\n",
        "            time.sleep(1)\n",
        "    return None\n",
        "\n",
        "# --- MAIN EXECUTION ---\n",
        "\n",
        "# Load gold standard examples for few-shot prompting\n",
        "gold_path = 'data/gold_standard_100.csv'\n",
        "if os.path.exists(gold_path):\n",
        "    df_gold = pd.read_csv(gold_path)\n",
        "    if 'review' not in df_gold.columns and 'text' in df_gold.columns: df_gold['review'] = df_gold['text']\n",
        "    examples = df_gold.sample(3).to_dict('records') if len(df_gold) >= 3 else df_gold.to_dict('records')\n",
        "\n",
        "    # Check if we already have the labels to avoid re-run\n",
        "    output_file = 'data/llm_labels_150.csv'\n",
        "    if os.path.exists(output_file):\n",
        "        print(f\"Found '{output_file}', skipping API calls.\")\n",
        "        df_unlabeled = pd.read_csv(output_file)\n",
        "    else:\n",
        "        # Load remaining unlabeled reviews (~150, select last 150 from movie_reviews_300.csv)\n",
        "        # Note: We use data/Movie_reviews.csv if xlsx not present\n",
        "        df_all = None\n",
        "        if os.path.exists('Movie_review.xlsx'): df_all = pd.read_excel('Movie_review.xlsx')\n",
        "        elif os.path.exists('data/Movie_reviews.csv'): df_all = pd.read_csv('data/Movie_reviews.csv')\n",
        "\n",
        "        if df_all is not None:\n",
        "            df_unlabeled = df_all.iloc[-150:].copy() if len(df_all) >= 150 else df_all.copy()\n",
        "            if 'text' in df_unlabeled.columns and 'review' not in df_unlabeled.columns:\n",
        "                df_unlabeled.rename(columns={'text': 'review'}, inplace=True)\n",
        "\n",
        "            # Query OpenRouter for each review\n",
        "            # Handle free tier requests per minute (RPM) limit of ~15\n",
        "            print(f\"Labeling {len(df_unlabeled)} reviews with LLM...\")\n",
        "            llm_labels = []\n",
        "            for i, row in enumerate(df_unlabeled.to_dict('records')):\n",
        "                if i > 0 and i % 10 == 0: time.sleep(60) # Crude RPM handle\n",
        "                label = query_openrouter(row.get('review', ''), examples)\n",
        "                llm_labels.append(label if label else \"Neutral\")\n",
        "                if i < 5: print(f\"Sample {i}: {llm_labels[-1]}\")\n",
        "\n",
        "            # Save LLM labels, in csv format with 'review' and 'label' columns\n",
        "            df_unlabeled['label'] = llm_labels\n",
        "            df_unlabeled[['review', 'label']].to_csv(output_file, index=False)\n",
        "            print(f\"Saved LLM labels to {output_file}\")\n",
        "else:\n",
        "    print(\"gold_standard_100.csv not found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8bca488",
      "metadata": {
        "id": "a8bca488"
      },
      "source": [
        "### Part 4.2: Noise Hunting (Cleanlab Logic)\n",
        "\n",
        "Train a Logistic Regression model on LLM-labeled data.\n",
        "Identify \"High Confidence Disagreements\" where the model is very confident (>0.80) but disagrees with the LLM label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "100fd6c7",
      "metadata": {
        "id": "100fd6c7"
      },
      "outputs": [],
      "source": [
        "def find_label_errors(llm_labels, model_probs, review_texts, threshold=0.90):\n",
        "    \"\"\"\n",
        "    Detects high-confidence disagreements between model predictions and LLM labels.\n",
        "    This implements Cleanlab logic: find cases where model is confident but disagrees with LLM.\n",
        "\n",
        "    Args:\n",
        "        llm_labels: List/array of labels from Gemini (numeric: 0=Negative, 1=Positive, 2=Neutral)\n",
        "        model_probs: Probability matrix from Logistic Regression (shape: N_samples, N_classes)\n",
        "        review_texts: List of review texts (for display)\n",
        "        threshold: Confidence threshold (default 0.90)\n",
        "\n",
        "    Returns:\n",
        "        list: List of dictionaries with suspicious review information\n",
        "              Each dict contains: 'index', 'text', 'llm_label', 'model_pred', 'confidence'\n",
        "    \"\"\"\n",
        "    # TODO: Get model predictions from probabilities\n",
        "\n",
        "\n",
        "    # TODO: Get model confidence (max probability) for each sample\n",
        "\n",
        "\n",
        "    # TODO: Convert llm_labels to numeric if they are strings\n",
        "    # Map 'Positive'->1, 'Negative'->0, 'Neutral'->2\n",
        "\n",
        "    # TODO: Find disagreements where:\n",
        "    #   Hint: disagreement_mask = (preds != llm_labels) & (confidences > threshold)\n",
        "\n",
        "    # TODO: Create list of suspicious reviews with all relevant information (llm label, model prediction, confidence)\n",
        "\n",
        "\n",
        "    # TODO: Sort by confidence (highest first) to find most egregious errors\n",
        "\n",
        "    # TODO: Return list of suspicious reviews\n",
        "    pass\n",
        "\n",
        "\n",
        "# TODO: Load LLM labels in dataframe\n",
        "\n",
        "# TODO: Vectorize LLM-labeled reviews (use same vectorizer from Task 3)\n",
        "\n",
        "# TODO: Train Logistic Regression on LLM-labeled data\n",
        "# Use same model configuration as Task 3 for consistency\n",
        "\n",
        "# TODO: Get probabilities on the same data (self-check), shape should be (N_samples, N_classes)\n",
        "\n",
        "# TODO: Find label errors using your function\n",
        "\n",
        "# TODO: Print top 5 suspicious reviews (if <5, print all)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c560870",
      "metadata": {
        "id": "5c560870"
      },
      "source": [
        "## Deliverables\n",
        "\n",
        "**Submission Checklist:**\n",
        "- [ ] Completed Jupyter Notebook with all tasks (Tasks 1-4)\n",
        "- [ ] Include your label-studio annotation interface screenshot.\n",
        "- [ ] gold_standard_100.csv\n",
        "- [ ] weak_labels_200.csv\n",
        "- [ ] llm_labels_150.json"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}